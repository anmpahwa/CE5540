{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1a30a0",
   "metadata": {},
   "source": [
    "# Lecture 19: Regression\n",
    "\n",
    "```\n",
    "Given a multivariate data, it can be summarised - statistically through measures of location, dispersion, shape, and association; and visually through scatter plots, line plots, bar charts, histograms, and box plots, among others. In this lecture, we will explore Regression as a tool to model relationships between multivariate data. \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Endogenous Variable\n",
    "\n",
    "The word endogenous comes from the Greek words endon (meaning \"within\") and genes (meaning \"produced\" or \"generated\"). Hence, an endogenous variable - also known as the dependent or response variable - is one whose value is determined within the system being studied. In the context of regression, it is the variable we aim to explain or predict based on its relationship with exogenous variables.\n",
    "\n",
    "## Exogenous Variables\n",
    "\n",
    "The word exogenous originates from the Greek exo (meaning \"outside\") and genes (meaning \"produced\" or \"generated\"). Hence, an exogenous variable - also known as an independent or predictor variable - is a variable whose value is determined outside the system being modeled and is not influenced by other variables within that model. In a regression context, exogenous variables are assumed to be known or given, and their influence on the endogenous variable is estimated through the regression process.\n",
    "\n",
    "## Examples\n",
    "\n",
    "- Travel Time Prediction on an Arterial Corridor\n",
    "\n",
    "    Endogenous Variable: travel time\n",
    "    \n",
    "    Exogenous Variables: traffic voume, signal timing parameters, time of day, etc.\n",
    "\n",
    "- Mode Choice Prediction for Commuter Trips\n",
    "\n",
    "    Endogenous Variable: mode choice\n",
    "    \n",
    "    Exogenous Variables: individual-specific parameters (socio-demographic and socio-economic variables), choice-specific parameters (time, cost, safety, and reliability, among others)\n",
    "\n",
    "- Freight Volume Prediction in a Logistics Network\n",
    "  \n",
    "    Endogenous Variable: freight volume\n",
    "\n",
    "    Exogenous Variables: supply parameters (industrial activity in the form of number of entities, floor space, labour force, etc.), network variables (extent of multimodal connectivity thorugh road, rail, air, and port), and demand parameters (accessibility to markets, level of urbanization, household income levels, etc.)\n",
    "\n",
    "\n",
    "## Linear Regression Model\n",
    "\n",
    "Linear regression is a fundamental statistical tool used to model the relationship between a dependent (endogenous) variable and one or more independent (exogenous) variables.\n",
    "\n",
    "### General Model\n",
    "\n",
    "The general linear regression model can be represented in the matrix form as, $\\mathbf{Y} = \\mathbf{X}\\bm{\\beta} + \\bm{\\epsilon}$,\n",
    "\n",
    "where, \n",
    "\n",
    "- $\\mathbf{Y}$ is the vector of observed dependent (endogenous) variable values\n",
    "\n",
    "- $\\mathbf{X}$ is the matrix of observed values of independent (exogenous) variables. Note that it includes a column of ones for the intercept.\n",
    "\n",
    "- $\\bm{\\beta}$ is the vector of regression coefficients\n",
    "\n",
    "- $\\bm{\\epsilon}$ is the vector of random errors (disturbances), capturing unobserved influences\n",
    "\n",
    "$$ \n",
    "\\mathbf{Y} = \n",
    "\\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "\\vdots \\\\\n",
    "Y_i \\\\\n",
    "\\vdots \\\\\n",
    "Y_n\n",
    "\\end{bmatrix}_{n \\times 1}\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & X_{11} & \\cdots & X_{1j} & \\cdots & X_{1m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & X_{i1} & \\cdots & X_{ij} & \\cdots & X_{im} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & X_{n1} & \\cdots & X_{nj} & \\cdots & X_{nm}\n",
    "\\end{bmatrix}_{n \\times (m+1)}\n",
    "\\bm{\\beta} = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_j \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix}_{(m+1) \\times 1}\n",
    "\\bm{\\epsilon} = \n",
    "\\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_i \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_n\n",
    "\\end{bmatrix}_{n \\times 1}\n",
    "$$\n",
    "\n",
    "Consequently, the expanded form renders,\n",
    "\n",
    "$$\n",
    "Y_i = \\beta_0 + \\beta_1X_{i1} + \\dots + \\beta_jX_{ij} + \\dots + \\beta_mX_{im} + \\epsilon\n",
    "$$\n",
    "\n",
    "where, \n",
    "\n",
    "- $Y_i$ is the actual observed outcome for the $i^{th}$ observation\n",
    "\n",
    "- $X_{ij}$ is the value of the $j^{th}$ independent variable for the $i^{th}$ observation\n",
    "\n",
    "- $\\beta_j$ is the regression coefficient associated with $j^{th}$ variable\n",
    "\n",
    "- $\\epsilon_i$ captures the unobserved factors affecting $Y_i$\n",
    "\n",
    "The regression coefficients are estimated by minimizing the sum of squared errors (SSE) as follows,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bm{\\hat{\\beta}} & = \\text{argmin} \\ \\text{SSE} \\\\\n",
    "\\bm{\\hat{\\beta}} & = \\text{argmin} \\ \\bm{\\epsilon}^\\text{T} \\bm{\\epsilon} \\\\\n",
    "\\bm{\\hat{\\beta}} & = \\text{argmin} \\ (\\mathbf{Y} - \\mathbf{X}\\bm{\\beta})^\\text{T} (\\mathbf{Y} - \\mathbf{X}\\bm{\\beta}) \\\\\n",
    "\\bm{\\hat{\\beta}} & = \\text{argmin} \\ (\\mathbf{Y}^\\top \\mathbf{Y} - 2\\bm{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{Y} + \\bm{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\bm{\\beta}) \\\\\n",
    "\\bm{\\hat{\\beta}} & = (\\mathbf{X}^\\text{T} \\mathbf{X})^{-1}\\mathbf{X}^\\text{T} \\mathbf{Y}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Consequently, the prediction is given by,\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{Y}} = \\mathbf{X}\\bm{\\hat{\\beta}}\n",
    "$$\n",
    "\n",
    "Equivalently,\n",
    "\n",
    "$$\n",
    "\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_{i1} + \\dots + \\hat{\\beta}_jX_{ij} + \\dots + \\hat{\\beta}_mX_{im}\n",
    "$$\n",
    "\n",
    "These predicted values represent the best linear approximation of the outcome based on the observed explanatory variables, under the classical regression assumptions.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "For linear regression to yield unbiased, consistent, and efficient estimators, the following assumptions must hold true,\n",
    "\n",
    "```{note}\n",
    "Ubiased: An estimator is said to be unbiased if its expected value matches the true parameter it aims to estimate.\n",
    "Consistent: An estimator is said to be consistent if it converges to the true parameter as the sample size increases.\n",
    "Efficient: Among all unbiased estimators, an estimator is efficient if it has the lowest possible variance.\n",
    "```\n",
    "\n",
    "- The relationship between the dependent variable and independent variables must inherently be linear in parameters. If the assumption of **linearity** does not hold true then the estimates are no longer unbiased.\n",
    "\n",
    "- The observations must be independently sampled leading to uncorrelated errors across observations. If the assumption of **independence** does not hold true then the estimates are no longer efficient.\n",
    "\n",
    "- The error terms must have constant variance for all observations, $\\text{Var}(\\epsilon_i | \\bf{X}) = \\sigma^2$. If the assumption of **homoskedasticity** does not hold ture then the estimates are no longer efficient.\n",
    "\n",
    "- The independent variables must not form perfect linear combinations of each other. If the assumption of **no multicollinearity** does not hold true then the model is no longer interpretable.\n",
    "\n",
    "- The error term has an expected value of zero given the explanatory variables, $\\text{E}(\\epsilon_i | \\bf{X}) = 0$. If the assumption of **zero conditional mean or errors** does not hold true the estimates are no longer unbiased."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
